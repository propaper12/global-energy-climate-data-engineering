import os
from pyspark.sql import SparkSession
from minio import Minio
from config import MINIO_ACCESS_KEY, MINIO_SECRET_KEY, MINIO_ENDPOINT, DB_CONFIG, logger

def get_spark_session():
    return SparkSession.builder \
        .appName("GECI_Spark_Dynamic_Pipeline") \
        .config("spark.jars.packages", "org.apache.hadoop:hadoop-aws:3.3.4,org.postgresql:postgresql:42.5.4") \
        .config("spark.hadoop.fs.s3a.endpoint", MINIO_ENDPOINT) \
        .config("spark.hadoop.fs.s3a.access.key", MINIO_ACCESS_KEY) \
        .config("spark.hadoop.fs.s3a.secret.key", MINIO_SECRET_KEY) \
        .config("spark.hadoop.fs.s3a.path.style.access", "true") \
        .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
        .getOrCreate()

def run_dynamic_etl():
    logger.info("ğŸš€ Dinamik Boru HattÄ± BaÅŸlatÄ±ldÄ±: Bronze -> Silver -> Gold")

    # 1. MinIO'ya baÄŸlanÄ±p Bronze iÃ§indeki TÃœM dosyalarÄ± listele
    try:
        minio_client = Minio(MINIO_ENDPOINT.replace("http://", ""), 
                             access_key=MINIO_ACCESS_KEY, 
                             secret_key=MINIO_SECRET_KEY, 
                             secure=False)
        
        objects = minio_client.list_objects("raw-data", prefix="bronze/", recursive=True)
        csv_files = [obj.object_name for obj in objects if obj.object_name.endswith('.csv')]
    except Exception as e:
        logger.error(f"âŒ MinIO baÄŸlantÄ± hatasÄ±: {e}")
        return

    if not csv_files:
        logger.warning("âš ï¸ Bronze klasÃ¶rÃ¼nde iÅŸlenecek CSV bulunamadÄ±. LÃ¼tfen Ingest iÅŸlemini Ã§alÄ±ÅŸtÄ±rÄ±n.")
        return

    spark = get_spark_session()
    logger.info(f"ğŸ“¦ MinIO'da {len(csv_files)} adet dosya bulundu. Ä°ÅŸlem baÅŸlÄ±yor...")

    # 2. Bulunan her bir dosyayÄ± sÄ±rayla iÅŸle
    for file_path in csv_files:
        # Dosya adÄ±nÄ± temizleyip tablo adÄ± Ã¼retiyoruz (Ã¶rn: "bronze/veri.csv" -> "veri")
        base_name = file_path.split('/')[-1].replace('.csv', '')
        table_name = base_name.replace('-', '_').replace(' ', '_').lower()
        
        bronze_s3_path = f"s3a://raw-data/{file_path}"
        
        try:
            df = spark.read.option("header", "true").option("inferSchema", "true").csv(bronze_s3_path)
            logger.info(f"ğŸ“¥ Okundu: {base_name}.csv")
        except Exception as e:
            logger.error(f"âŒ Spark okuma hatasÄ± ({bronze_s3_path}): {e}")
            continue

        # Kolon isimlerindeki boÅŸluk ve parantezleri temizle (Parquet ve Postgres kurallarÄ±)
        for c in df.columns:
            clean_col = c.replace(" ", "_").replace("(", "").replace(")", "").replace("-", "_").replace("%", "pct")
            df = df.withColumnRenamed(c, clean_col)
        
        # 3. SILVER'A YAZ (Parquet formatÄ±nda)
        silver_path = f"s3a://raw-data/silver/{table_name}.parquet"
        df.write.mode("overwrite").parquet(silver_path) 
        logger.info(f"ğŸ¥ˆ Silver (Parquet) YazÄ±ldÄ±: {table_name}.parquet")

        # 4. GOLD'A YAZ (Postgres) - Her CSV kendi adÄ±yla tablo olur
        df.write \
            .format("jdbc") \
            .option("url", f"jdbc:postgresql://{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['database']}") \
            .option("dbtable", table_name) \
            .option("user", DB_CONFIG['user']) \
            .option("password", DB_CONFIG['password']) \
            .option("driver", "org.postgresql.Driver") \
            .mode("overwrite") \
            .save() 
        
        logger.info(f"ğŸ† Gold (Postgres) GÃ¼ncellendi: Tablo AdÄ± -> {table_name}")

    spark.stop()
    logger.info("ğŸ‰ TÃ¼m veriler baÅŸarÄ±yla iÅŸlendi ve veri ambarÄ±na aktarÄ±ldÄ±!")

if __name__ == "__main__":
    run_dynamic_etl()
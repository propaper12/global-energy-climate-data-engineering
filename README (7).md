
# ğŸŒ GECI: Global Energy & Climate Intelligence Hub

![Python](https://img.shields.io/badge/Python-3.10%2B-blue?style=flat-square&logo=python)
![Apache Spark](https://img.shields.io/badge/Apache_Spark-PySpark-orange?style=flat-square&logo=apachespark)
![PostgreSQL](https://img.shields.io/badge/PostgreSQL-Data_Warehouse-336791?style=flat-square&logo=postgresql)
![MinIO](https://img.shields.io/badge/MinIO-Data_Lake-C7202C?style=flat-square&logo=minio)
![Docker](https://img.shields.io/badge/Docker-Containerization-2496ED?style=flat-square&logo=docker)
![Streamlit](https://img.shields.io/badge/Streamlit-Web_App-FF4B4B?style=flat-square&logo=streamlit)
![Machine Learning](https://img.shields.io/badge/Machine_Learning-XGBoost_%7C_Sklearn-yellow?style=flat-square)

## ğŸ“Œ Projenin AmacÄ± (About The Project)
**
**GECI (Global Energy & Climate Intelligence Hub)**, kÃ¼resel Ã¶lÃ§ekte enerji tÃ¼ketimi, yenilenebilir enerji dÃ¶nÃ¼ÅŸÃ¼mÃ¼ ve karbon emisyonu verilerini analiz eden, uÃ§tan uca tasarlanmÄ±ÅŸ kapsamlÄ± bir **Veri MÃ¼hendisliÄŸi ve Yapay Zeka platformudur**.

Bu proje, yalnÄ±zca gÃ¶rselleÅŸtirme odaklÄ± bir dashboard sunmakla kalmaz; aynÄ± zamanda ham verinin toplanmasÄ±ndan (**Data Ingestion**), iÅŸlenmesine (**ETL**), veri ambarÄ±na aktarÄ±lmasÄ±na ve makine Ã¶ÄŸrenmesi modelleri ile ileriye dÃ¶nÃ¼k tahminler Ã¼retilmesine kadar tÃ¼m **veri yaÅŸam dÃ¶ngÃ¼sÃ¼nÃ¼ (Data Lifecycle)** otomatik hale getiren Ã¶lÃ§eklenebilir bir mimari sunar.

## ğŸ—ï¸ Veri Mimarisi (Data Architecture)

Proje altyapÄ±sÄ±, modern veri mÃ¼hendisliÄŸi standartlarÄ±na uygun olarak **Madalyon Mimarisi (Medallion Architecture)** yaklaÅŸÄ±mÄ±yla tasarlanmÄ±ÅŸtÄ±r.

1.  ğŸ¥‰ **Bronze Layer (MinIO / S3)**  
    Ham CSV veri setleri (Our World in Data, Kaggle vb.) yapÄ±sal olmayan formatta veri gÃ¶lÃ¼ne (Data Lake) aktarÄ±lÄ±r. Bu sÃ¼reÃ§ `ingest_to_s3.py` scripti ile otomatik olarak gerÃ§ekleÅŸtirilir.
    
2.  ğŸ¥ˆ **Silver Layer (Apache Spark)**  
    PySpark kullanÄ±larak ham veriler okunur, ÅŸema dÃ¼zeltmeleri ve veri temizliÄŸi uygulanÄ±r. PerformanslÄ± okuma ve analiz iÃ§in `Parquet` formatÄ±nda tekrar MinIO Ã¼zerine yazÄ±lÄ±r (`etl_spark_to_db.py`).
    
3.  ğŸ¥‡ **Gold Layer (PostgreSQL)**  
    TemizlenmiÅŸ ve iÅŸ kurallarÄ± uygulanmÄ±ÅŸ veri, veri ambarÄ±na (Data Warehouse) iliÅŸkisel tablolar halinde aktarÄ±lÄ±r. Makine Ã¶ÄŸrenmesi modelleri ve Streamlit tabanlÄ± BI uygulamalarÄ± yalnÄ±zca bu katmandan beslenir.

## 
## Temel Ã–zellikler (Key Features)

-   **âš¡ PySpark ETL Motoru:**  
    Milyonlarca satÄ±rlÄ±k veriyi yÃ¼ksek performansla iÅŸleyebilen, idempotent ve Ã¶lÃ§eklenebilir veri boru hattÄ±.
    
-   **ğŸ“¡ Hibrit Veri FÃ¼zyonu:**  
    Tarihsel veri setlerinin, **NASA POWER API** ve meteorolojik sensÃ¶r verileriyle gerÃ§ek zamanlÄ± olarak birleÅŸtirilmesi.
    
-   **ğŸ”® AI 2040 Projeksiyonu (XGBoost & Ridge):**  
    Ãœlke bazlÄ± eÄŸitilen modeller ile 2040 yÄ±lÄ±na kadar yenilenebilir enerji Ã¼retim tahminleri.
    
-   **ğŸ›ï¸ Politika SimÃ¼latÃ¶rÃ¼ (Random Forest):**  
    Enerji Ã¼retim miksindeki deÄŸiÅŸimlerin karbon ayak izine etkisini anlÄ±k olarak simÃ¼le eden karar destek sistemi.
    
-   **ğŸ§ª Otomatik Kalite Kontrol (Unit Testing):**  
    `unittest` altyapÄ±sÄ± ile API baÄŸlantÄ±larÄ±, veri doÄŸruluÄŸu ve sistem saÄŸlÄ±ÄŸÄ±nÄ±n otomatik test edilmesi.


## ğŸ› ï¸ Teknoloji YÄ±ÄŸÄ±nÄ± (Tech Stack)

-   **Veri Ä°ÅŸleme:** Apache Spark (PySpark), Pandas, PyArrow
    
-   **AltyapÄ± & Depolama:** Docker, Docker Compose, PostgreSQL, MinIO
    
-   **Makine Ã–ÄŸrenmesi:** Scikit-Learn, XGBoost, Joblib
    
-   **GÃ¶rselleÅŸtirme:** Streamlit, Plotly
    
-   **DiÄŸer AraÃ§lar:** SQLAlchemy, Requests, Python-Dotenv
## ğŸ“‚ Proje YapÄ±sÄ± (Directory Structure)
```text
ğŸ“¦ GECI_Project
 â”£ ğŸ“œ docker-compose.yml       # DB ve MinIO konteyner altyapÄ±sÄ±
 â”£ ğŸ“œ requirements.txt         # Python baÄŸÄ±mlÄ±lÄ±klarÄ±
 â”£ ğŸ“œ .env                     # Gizli ortam deÄŸiÅŸkenleri
 â”£ ğŸ“œ config.py                # KonfigÃ¼rasyon ve Loglama yÃ¶netimi
 â”£ ğŸ“œ utils.py                 # DB baÄŸlantÄ±larÄ± ve NASA API yardÄ±mcÄ± fonksiyonlarÄ±
 â”£ ğŸ“œ test_app.py              # Kalite Kontrol (QA) Unit Testleri
 â”ƒ # --- Veri Boru HattÄ± (Data Pipeline) ---
 â”£ ğŸ“œ ingest_to_s3.py          # Lokalden MinIO Bronze katmanÄ±na veri aktarÄ±mÄ±
 â”£ ğŸ“œ etl_spark_to_db.py       # PySpark ETL motoru (Bronze -> Silver -> Gold)
 â”£ ğŸ“œ train_models.py          # Toplu ve spesifik AI model eÄŸitim scripti
 â”ƒ # --- KullanÄ±cÄ± ArayÃ¼zÃ¼ (Streamlit) ---
 â”£ ğŸ“œ Home.py                  # Ana gÃ¶sterge paneli ve Pipeline KontrolcÃ¼sÃ¼
 â”£ ğŸ“‚ pages/                   # ModÃ¼ler arayÃ¼z sayfalarÄ±
 â”ƒ â”£ ğŸ“œ 1_Komuta_Merkezi.py    # Operasyonel Ä°zleme & NASA verileri
 â”ƒ â”£ ğŸ“œ 2_Fosil_vs_Yesil.py    # Makas analizi ve geÃ§iÅŸ momentumu
 â”ƒ â”£ ğŸ“œ 3_Hava_ve_Enerji.py    # Atmosferik Zeka
 â”ƒ â”£ ğŸ“œ 4_AI_Projeksiyonu.py   # AI ile 2040 tahminleri
 â”ƒ â”£ ğŸ“œ 5_Politika_Simulatoru.py # Random Forest destekli karbon simÃ¼latÃ¶rÃ¼
 â”ƒ â”£ ğŸ“œ 6_Veri_Kesfi.py        # KÃ¼resel korelasyonlar ve Liderlik tablolarÄ±
 â”ƒ â”£ ğŸ“œ 7_Kalite_Kontrol.py    # Sistem saÄŸlÄ±k paneli ve test arayÃ¼zÃ¼
 â”ƒ â”— ğŸ“œ 8_Proje_Ozeti.py       # DokÃ¼mantasyon
 â”— ğŸ“‚ models/                  # EÄŸitilmiÅŸ .pkl formatlÄ± yapay zeka modelleri
 ```
 ## âš™ï¸ Kurulum ve Ã‡alÄ±ÅŸtÄ±rma (Quick Start)

**1. Repoyu KlonlayÄ±n**

Bash

```
git clone [https://github.com/kullanici_adiniz/GECI-Energy-Hub.git](https://github.com/kullanici_adiniz/GECI-Energy-Hub.git)
cd GECI-Energy-Hub

```

**2. Gerekli Paketleri YÃ¼kleyin**

Bash

```
pip install -r requirements.txt

```

**3. AltyapÄ±yÄ± AyaÄŸa KaldÄ±rÄ±n (Docker)** PostgreSQL ve MinIO sunucularÄ±nÄ± baÅŸlatmak iÃ§in:

Bash

```
docker-compose up -d

```
## 
## ğŸ“¥ Veri YÃ¼kleme â€“ MinIO (Data Lake) KullanÄ±mÄ±

Projede yer alan tÃ¼m **ham veri setleri**, MinIO Ã¼zerinde oluÅŸturulan **`raw-data` bucketâ€™Ä±** iÃ§erisine yÃ¼klenmelidir. Bu yapÄ±, veri gÃ¶lÃ¼ mimarisinin Bronze katmanÄ±nÄ± temsil eder.

### AdÄ±m AdÄ±m Veri YÃ¼kleme SÃ¼reci

1.  MinIO web arayÃ¼zÃ¼ne giriÅŸ yapÄ±n.
    
    -   **KullanÄ±cÄ± adÄ±:** admin
        
    -   **Åifre:** minio_password
        
2.  `raw-data` isimli bucketâ€™Ä± oluÅŸturun.
### 4. AdÄ±m: Spark ETL SÃ¼recini BaÅŸlat


 Åimdi MinIO'daki o ham verileri alÄ±p, temizleyip Postgres veritabanÄ±na atacaÄŸÄ±z.

Bash

```
docker exec -it geci_dashboard python ingest_to_s3.py

 docker exec -it geci_dashboard python etl_spark_to_db.py

docker restart geci_dashboard

docker-compose down ile sistemi durdurun.
```

## â™»ï¸ Sistemi SÄ±fÄ±rlama ve Yeniden Kurulum

Docker, PostgreSQL ve MinIO verileri kalÄ±cÄ± hale getirmek iÃ§in **volume** yapÄ±sÄ±nÄ± kullanÄ±r. Sistemi tamamen sÄ±fÄ±rlamak ve temiz kurulum yapmak iÃ§in:
```
docker-compose down -v
```

ArdÄ±ndan sistemi tekrar baÅŸlatmak iÃ§in:
```
docker-compose up -d
```

Bu iÅŸlem, ortamÄ± temiz bir ÅŸekilde yeniden oluÅŸturmanÄ±za olanak tanÄ±r.

```
